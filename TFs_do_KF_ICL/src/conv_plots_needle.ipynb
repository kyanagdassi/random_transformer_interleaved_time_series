{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Convergence Plots for Learning Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.family'] = 'Serif'\n",
    "plt.rcParams['font.serif'] = 'Liberation Serif'\n",
    "import pickle\n",
    "import os\n",
    "print(os.getcwd())\n",
    "import time\n",
    "import gc\n",
    "import torch\n",
    "import bisect\n",
    "\n",
    "from conv_plots_funcs import train_conv_plots\n",
    "\n",
    "#import empirical cdf\n",
    "# import sys\n",
    "# sys.path.append(os.path.abspath('../../src'))\n",
    "\n",
    "from data_processing import gen_ckpt_steps, move_dict_to_device, get_other_err, get_mop_ratios_ckpt, compute_ratio\n",
    "# sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "from check_ecdf import get_empirical_cdf\n",
    "from pretrain_loss import compute_pseudo_pred_avg_pipeline, get_multi_sys_ys, compute_pseudo_pred_errs, compute_pseudo_pred_errs_needle_in_haystack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valA = \"ortho\"\n",
    "valC = \"_ident_C\"\n",
    "state_dim = 5\n",
    "ckpt_step = 8\n",
    "batch_size = 512 #512 #4096\n",
    "gpus=2\n",
    "num_val_sys = 25\n",
    "experiment = \"250112_043028.07172b_multi_sys_trace_ortho_state_dim_5_ident_C_lr_1.584893192461114e-05_num_train_sys_40000\" # experiment to load\n",
    "datasource = \"val\"\n",
    "nope = False\n",
    "single_system = True\n",
    "zero_cut = False\n",
    "\n",
    "\n",
    "compute_more_ckpts = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kal_ckpt = 141000 #60000 #81000 #28500\n",
    "train_conv_plots([experiment], [\"Identity\"], [kal_ckpt], valA, valC, num_val_sys, compute_more_ckpts, None, 3000, 180000, 3000, state_dim, single_system=single_system, nope=nope, batch_size=batch_size, gpus=gpus, zero_cut=zero_cut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero Cut Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valA = \"gaussA\"\n",
    "valC = \"_gauss_C\"\n",
    "batch_size = 512 #512 #4096\n",
    "gpus=2\n",
    "num_val_sys = 25\n",
    "experiment_zero_train = \"250127_001511.3ac954_multi_sys_trace_zero_cut_gaussA_state_dim_10_gauss_C_lr_1.584893192461114e-05_num_train_sys_40000\" # experiment to load\n",
    "experiment_multi_train = \"250114_202420.3c1184_multi_sys_trace_gaussA_state_dim_10_gauss_C_lr_1.584893192461114e-05_num_train_sys_40000\"\n",
    "kal_ckpt_zero_train = 90000 \n",
    "# kal_ckpt_multi_train = 30000 #180000\n",
    "datasource = \"val\"\n",
    "state_dim = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kal_err_zero_train = get_other_err(valA, valC, kal_ckpt_zero_train, experiment_zero_train, \"Kalman_rem\", state_dim, zero_cut=True)\n",
    "\n",
    "tf_err_zero_train, pred_ckpt = get_mop_ratios_ckpt(valA, valC, kal_ckpt_zero_train, experiment_zero_train, state_dim, zero_cut=True)\n",
    "\n",
    "ols_irs = []\n",
    "for ir in range(1, 4):\n",
    "    ols_ir = get_other_err(valA, valC, kal_ckpt_zero_train, experiment_zero_train, f\"OLS_ir_{ir}\", state_dim, zero_cut=True)\n",
    "    ols_irs.append(compute_ratio(None, ols_ir, kal_err_zero_train).cpu().numpy())\n",
    "\n",
    "tf_err_rat_zero_train = compute_ratio(None, tf_err_zero_train, kal_err_zero_train)\n",
    "tf_err_rat_zero_train = tf_err_rat_zero_train.cpu().numpy()\n",
    "print(\"shape of tf_err_rat_zero_train: \", tf_err_rat_zero_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save zero train data\n",
    "os.makedirs(\"../outputs/GPT2/\" + experiment_zero_train + \"/context\", exist_ok=True)\n",
    "np.save(\"../outputs/GPT2/\" + experiment_zero_train + f\"/context/tf_err_rat_zero_train_step_{kal_ckpt_zero_train}.npy\", tf_err_rat_zero_train)\n",
    "np.save(\"../outputs/GPT2/\" + experiment_zero_train + \"/context/ols_irs_zero_train.npy\", ols_irs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_multi_train = \"250418_125901.8d6b22_multi_sys_trace_ortho_haar_state_dim_5_ident_C_lr_1.584893192461114e-05_num_train_sys_40000\"\n",
    "valA = \"ortho_haar\"\n",
    "valC = \"_ident_C\"\n",
    "state_dim = 5\n",
    "batch_size = 512\n",
    "gpus=1\n",
    "\n",
    "max_ckpt = 183000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for kal_ckpt_multi_train in np.arange(1000, max_ckpt, 1000):\n",
    "    if kal_ckpt_multi_train < max_ckpt:\n",
    "        train_conv = True\n",
    "    else:\n",
    "        train_conv = False\n",
    "\n",
    "    tf_err_multi_train, pred_ckpt = get_mop_ratios_ckpt(valA, valC, kal_ckpt_multi_train, experiment_multi_train, state_dim, zero_cut=True, train_conv=train_conv)\n",
    "\n",
    "    if tf_err_multi_train is None:\n",
    "        print(\"No data for step: \", kal_ckpt_multi_train)\n",
    "        continue\n",
    "    # an_sim_err_multi_train = get_other_err(valA, valC, kal_ckpt_multi_train, experiment_multi_train, \"Analytical_Simulation\", state_dim, zero_cut=True, train_conv=train_conv)\n",
    "    # an_sim_err_multi_train = compute_ratio(None, an_sim_err_multi_train, kal_err_zero_train).cpu().numpy()\n",
    "\n",
    "    # ols_irs = []\n",
    "    # for ir in range(1, 4):\n",
    "    #     ols_ir = get_other_err(valA, valC, kal_ckpt_multi_train, experiment_zero_train, f\"OLS_ir_{ir}\", state_dim, zero_cut=True)\n",
    "    #     ols_irs.append(compute_ratio(None, ols_ir, kal_err_zero_train).cpu().numpy())\n",
    "\n",
    "    tf_err_rat_multi_train = compute_ratio(None, tf_err_multi_train, kalman_err=None)\n",
    "    tf_err_rat_multi_train = tf_err_rat_multi_train.cpu().numpy()\n",
    "    print(\"shape of tf_err_rat_multi_train: \", tf_err_rat_multi_train.shape)\n",
    "\n",
    "    #save the data\n",
    "    os.makedirs(\"../outputs/GPT2/\" + experiment_multi_train + \"/context\", exist_ok=True)\n",
    "    np.save(\"../outputs/GPT2/\" + experiment_multi_train + f\"/context/tf_err_rat_multi_train_step_{kal_ckpt_multi_train}.npy\", tf_err_rat_multi_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_err_rat_zero_train = np.load(\"../outputs/GPT2/\" + experiment_zero_train + \"/context/tf_err_rat_zero_train.npy\")\n",
    "# ols_irs = np.load(\"../outputs/GPT2/\" + experiment_zero_train + \"/context/ols_irs_zero_train.npy\")\n",
    "\n",
    "# print(\"tf_err_rat_zero_train: \", tf_err_rat_zero_train)\n",
    "\n",
    "ckpt_steps = []\n",
    "tf_err_multi_trains = []\n",
    "for kal_ckpt_multi_train in np.arange(1000, max_ckpt, 1000):\n",
    "\n",
    "    try:\n",
    "        tf_err_multi_train = np.load(\"../outputs/GPT2/\" + experiment_multi_train + f\"/context/tf_err_rat_multi_train_step_{kal_ckpt_multi_train}.npy\")\n",
    "        print(f\"kal_ckpt_multi_train: {kal_ckpt_multi_train}\")\n",
    "        # print(f\"kal_ckpt_multi_train: {kal_ckpt_multi_train}, tf_err_multi_train: {tf_err_multi_train}\")\n",
    "        tf_err_multi_trains.append(tf_err_multi_train)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found for kal_ckpt_multi_train: {kal_ckpt_multi_train}\")\n",
    "        continue\n",
    "\n",
    "    ckpt_steps.append(kal_ckpt_multi_train)\n",
    "\n",
    "# tf_err_rat_multi_train_3k = np.load(\"../outputs/GPT2/\" + experiment_multi_train + \"/context/tf_err_rat_multi_train_step_3000.npy\")\n",
    "# tf_err_rat_multi_train_30k = np.load(\"../outputs/GPT2/\" + experiment_multi_train + \"/context/tf_err_rat_multi_train_step_30000.npy\")\n",
    "# tf_err_rat_multi_train_96k = np.load(\"../outputs/GPT2/\" + experiment_multi_train + \"/context/tf_err_rat_multi_train_step_96000.npy\")\n",
    "# tf_err_rat_multi_train_180k = np.load(\"../outputs/GPT2/\" + experiment_multi_train + \"/context/tf_err_rat_multi_train_step_180000.npy\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"batch_size: \", batch_size)\n",
    "print(\"gpus: \", gpus)\n",
    "print(\"batch_size*kal_ckpt_multi_train: \", batch_size*kal_ckpt_multi_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_err_rat_zero_train -= 1\n",
    "for ir in range(3):\n",
    "    ols_irs[ir] -= 1\n",
    "\n",
    "for i in range(len(tf_err_multi_trains)):\n",
    "    tf_err_multi_trains[i] -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scientific_notation(value):\n",
    "    \"\"\"\n",
    "    Returns the exponent and value of the scientific notation of an integer.\n",
    "    \n",
    "    Parameters:\n",
    "    value (int): The integer to convert to scientific notation.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: A tuple containing the value and exponent of the scientific notation.\n",
    "    \"\"\"\n",
    "    if value == 0:\n",
    "        return (0, 0)\n",
    "    \n",
    "    exponent = 0\n",
    "    while abs(value) >= 10:\n",
    "        value /= 10\n",
    "        exponent += 1\n",
    "    while abs(value) < 1:\n",
    "        value *= 10\n",
    "        exponent -= 1\n",
    "\n",
    "    value = round(value, 2)\n",
    "    \n",
    "    return (value, exponent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pseudo predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_train import generate_interleaved_traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasource = \"train\"\n",
    "multi_sys_ys, sys_choices_per_config, seg_starts_per_config, sys_inds_per_config, real_seg_lens_per_config, sys_dict_per_config = get_multi_sys_ys(datasource, hay_len=1, interleave_style=\"needle_in_haystack\")\n",
    "\n",
    "pseudo_pred_errs = compute_pseudo_pred_errs_needle_in_haystack(multi_sys_ys, seg_starts_per_config, real_seg_lens_per_config, sys_choices_per_config, zero_cut=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"sum of pseudo_pred_errs: {np.sum(pseudo_pred_errs)}\")\n",
    "print(f\"pseudo_pred_errs[0,0,0]: {pseudo_pred_errs[39000,0,6]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the pseudo_pred_errs to a file\n",
    "os.makedirs(f\"/data/shared/ICL_Kalman_Experiments/train_and_test_data/ortho_haar/\", exist_ok=True)\n",
    "with open(f\"/data/shared/ICL_Kalman_Experiments/train_and_test_data/ortho_haar/train_irrelevant_tokens_new_hay_insert_pseudo_pred_errs_ortho_haar_ident_C_haystack_len_1.pkl\", 'wb') as f:\n",
    "    pickle.dump(pseudo_pred_errs, f)\n",
    "    print(f\"Saved pseudo_pred_errs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the pseudo_pred_errs from a file\n",
    "with open(f\"/data/shared/ICL_Kalman_Experiments/train_and_test_data/ortho_haar/val_zero_cut_pseudo_pred_errs_ortho_haar_ident_C_haystack_len_.pkl\", 'rb') as f:\n",
    "    pseudo_pred_errs = pickle.load(f)\n",
    "    print(f\"Loaded pseudo_pred_errs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import BoundaryNorm, ListedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "context_len = 251\n",
    "\n",
    "# colors = ['#000000', '#005CAB', '#E31B23', '#FFC325', '#00A651', '#9B59B6', '#FF69B4', '#8B4513']\n",
    "\n",
    "#make a colormap for len(tf_err_multi_trains) + 4 colors\n",
    "# colors = plt.cm.viridis(np.linspace(0, 1, len(tf_err_multi_trains) + 5))\n",
    "# print(np.linspace(0, 1, len(tf_err_multi_trains) + 5))\n",
    "# intervals = np.linspace(0, 60, len(tf_err_multi_trains))\n",
    "# intervals = (0.97)**(intervals)\n",
    "# # intervals -= intervals.min()\n",
    "# # intervals = 1 - intervals\n",
    "# #reverse the intervals\n",
    "# intervals = intervals[::-1]\n",
    "# print(intervals)\n",
    "# start = 0.01\n",
    "# stop = 0.99\n",
    "# eps = 0\n",
    "# number_of_lines=len(tf_err_multi_trains)\n",
    "# inter = start\n",
    "# intervals = []\n",
    "# for i in range(number_of_lines):\n",
    "#     intervals.append(inter)\n",
    "#     inter += eps\n",
    "#     if i % 25 == 0:\n",
    "#         eps += 0.015\n",
    "\n",
    "# #add 1 to every element in intervals\n",
    "# # intervals = np.array(intervals)\n",
    "# # intervals -= 0.2\n",
    "# # intervals = intervals/np.max(intervals)\n",
    "# # print(intervals)\n",
    "\n",
    "frac = 0.4\n",
    "intervals = np.linspace(0,1, int(frac*len(tf_err_multi_trains))+1)\n",
    "intervals = np.array(intervals)\n",
    "intervals += 0.4\n",
    "print(intervals)\n",
    "print(len(intervals))\n",
    "\n",
    "cmap = plt.cm.Greys\n",
    "# norm = BoundaryNorm(intervals, cmap.N)\n",
    "\n",
    "\n",
    "colors_1 = cmap(intervals)\n",
    "\n",
    "intervals = np.linspace(0,1, int((1-frac)*len(tf_err_multi_trains))+1)\n",
    "print(intervals)\n",
    "\n",
    "cmap2 = plt.cm.magma\n",
    "# norm2 = BoundaryNorm(intervals, cmap2.N)\n",
    "colors_2 = cmap2(intervals)\n",
    "\n",
    "\n",
    "combined_colors = np.vstack((colors_1, colors_2))\n",
    "\n",
    "#add a single dark neon blue color to the middle of the combined colors\n",
    "add_color = np.array([0.0, 0.5, 1.0, 1.0])\n",
    "\n",
    "\n",
    "# After you have filled examples_list:\n",
    "target_examples = 6.25e7\n",
    "\n",
    "# multiply all elements of ckpt_steps by batch_size*gpus to get examples_list_orig\n",
    "examples_list_orig = np.array(ckpt_steps) * batch_size * gpus\n",
    "\n",
    "# Find the index where examples_list is closest to target_examples\n",
    "insert_idx = np.argmin(np.abs(np.array(examples_list_orig) - target_examples))\n",
    "\n",
    "\n",
    "# Insert the dark blue color at the correct position\n",
    "combined_colors = np.insert(combined_colors, insert_idx, add_color, axis=0)\n",
    "\n",
    "combined_cmap = ListedColormap(combined_colors)\n",
    "\n",
    "combined_intervals = np.linspace(0, 1, len(combined_colors))\n",
    "\n",
    "\n",
    "# zero_steps = scientific_notation(2048*kal_ckpt_zero_train)\n",
    "# steps_180k = scientific_notation(180000*batch_size*gpus)\n",
    "# steps_96k = scientific_notation(96000*batch_size*gpus)\n",
    "# steps_30k = scientific_notation(30000*batch_size*gpus)\n",
    "# steps_3k = scientific_notation(3000*batch_size*gpus)\n",
    "\n",
    "# ax.plot(np.arange(context_len), tf_err_rat_zero_train[1], label=f\"Zero-cut Trained at {zero_steps[0]}e{zero_steps[1]} examples\", linewidth=2, marker=\"x\", markersize=2, zorder=5, color = colors[1])#linestyle=\"--\")\n",
    "# ax.fill_between(np.arange(context_len), tf_err_rat_zero_train[0], tf_err_rat_zero_train[2], alpha=0.2, color = colors[1])\n",
    "\n",
    "# ax.plot(np.arange(context_len), tf_err_rat_multi_train_180k[1], label=f\"Multi-cut Trained at {steps_180k[0]}e{steps_180k[1]} examples\", linewidth=2, marker=\"x\", markersize=2, zorder=4, color=colors[0])#, linestyle=\"-.\")\n",
    "# ax.fill_between(np.arange(context_len), tf_err_rat_multi_train_180k[0], tf_err_rat_multi_train_180k[2], alpha=0.2, color=colors[0])\n",
    "\n",
    "# ax.plot(np.arange(context_len), tf_err_rat_multi_train_96k[1], label=f\"Multi-cut Trained at {steps_96k[0]}e{steps_96k[1]} examples\", linewidth=2, marker=\"x\", markersize=2, zorder=3, color=colors[5])#, linestyle=\"-.\")\n",
    "# ax.fill_between(np.arange(context_len), tf_err_rat_multi_train_96k[0], tf_err_rat_multi_train_96k[2], alpha=0.2, color=colors[5])\n",
    "\n",
    "# ax.plot(np.arange(context_len), tf_err_rat_multi_train_30k[1], label=f\"Multi-cut Trained at {steps_30k[0]}e{steps_30k[1]} examples\", linewidth=2, marker=\"x\", markersize=2, zorder=2, color=colors[7])#, linestyle=\"-.\")\n",
    "# ax.fill_between(np.arange(context_len), tf_err_rat_multi_train_30k[0], tf_err_rat_multi_train_30k[2], alpha=0.2, color=colors[7])\n",
    "\n",
    "\n",
    "# ax.plot(np.arange(context_len), tf_err_rat_multi_train_3k[1], label=f\"Multi-cut Trained at {steps_3k[0]}e{steps_3k[1]} examples\", linewidth=2, marker=\"x\", markersize=2, zorder=2, color=colors[6])#, linestyle=\"-.\")\n",
    "# ax.fill_between(np.arange(context_len), tf_err_rat_multi_train_3k[0], tf_err_rat_multi_train_3k[2], alpha=0.2, color=colors[6])\n",
    "\n",
    "# ax.plot(np.arange(context_len), an_sim_err_multi_train[1], label=\"Analytical Simulation\", linewidth=2, marker=\".\", markersize=10)\n",
    "# ax.fill_between(np.arange(context_len), an_sim_err_multi_train[0], an_sim_err_multi_train[2], alpha=0.15)\n",
    "\n",
    "# for ir in range(1, 4):\n",
    "#     ax.plot(np.arange(context_len), ols_irs[ir-1][1], label=f\"OLS-{ir}\", linewidth=2, marker=\".\", markersize=2, color=colors[ir+1])\n",
    "#     ax.fill_between(np.arange(context_len), ols_irs[ir-1][0], ols_irs[ir-1][2], alpha=0.2, color=colors[ir+1])\n",
    "\n",
    "# steps = np.arange(3000, 183000, 3000)\n",
    "# examples_list = []\n",
    "# colors = colors_1\n",
    "# color_count = 0\n",
    "# for i in range(len(tf_err_multi_trains)):\n",
    "#     print(f\"i: {i}, color_count: {color_count}\")\n",
    "#     if i > frac*len(tf_err_multi_trains):\n",
    "#         colors = colors_2\n",
    "#         if color_count == i:\n",
    "#             color_count = 0\n",
    "#     examples = steps[i]*batch_size*gpus\n",
    "#     examples_list.append(examples)\n",
    "#     examples = scientific_notation(examples)\n",
    "#     ax.plot(np.arange(context_len), tf_err_multi_trains[i][1], label=f\"Multi-cut Trained at {examples[0]}e{examples[1]} examples\", linewidth=2, marker=\"x\", markersize=2, color=colors[color_count])\n",
    "#     color_count += 1\n",
    "\n",
    "\n",
    "# steps = np.arange(1000, max_ckpt, 1000)\n",
    "examples_list = []\n",
    "color_count = 0\n",
    "for i in range(len(tf_err_multi_trains)):\n",
    "    print(f\"i: {i}, color_count: {color_count}\")\n",
    "    examples = ckpt_steps[i]*batch_size*gpus\n",
    "    examples_list.append(examples)\n",
    "    examples = scientific_notation(examples)\n",
    "    ax.plot(np.arange(context_len), tf_err_multi_trains[i][1], label=\"_no_legend_\", linewidth=2, marker=\".\", markersize=2, color=combined_colors[color_count])\n",
    "    if i == insert_idx:\n",
    "\n",
    "        fig.text(0.66,0.17, f\"{examples[0]}e{examples[1]} ex\", fontweight=10)\n",
    "    color_count += 1\n",
    "\n",
    "    # ax.fill_between(np.arange(context_len), tf_err_multi_trains[i][0], tf_err_multi_trains[i][2], alpha=0.2, color=colors[i+5])\n",
    "\n",
    "#plot the pseudo prediction errors\n",
    "#take the median of the pseudo prediction errors\n",
    "pseudo_pred_meds = np.median(pseudo_pred_errs, axis=1)\n",
    "print(f\"pseudo_pred_meds[0,2]: {pseudo_pred_meds[0,2]}\")\n",
    "print(f\"shape of pseudo_pred_meds: {pseudo_pred_meds.shape}\")\n",
    "pseudo_pred_qs = np.quantile(pseudo_pred_meds, [0.25, 0.5, 0.75], axis=0)\n",
    "print(f\"shape of pseudo_pred_qs: {pseudo_pred_qs.shape}\")\n",
    "print(f\"pseudo_pred_qs: {pseudo_pred_qs[1]}\")\n",
    "# ax.plot(np.arange(2, context_len), pseudo_pred_qs[1,2:], label=\"Pseudoinv pred.\", linewidth=2, linestyle=\":\", markersize=5, color='brown', zorder=100)\n",
    "\n",
    "ax.errorbar(\n",
    "    np.arange(2, context_len), pseudo_pred_qs[1,2:],\n",
    "    yerr=[pseudo_pred_qs[1,2:] - pseudo_pred_qs[0,2:], pseudo_pred_qs[2,2:] - pseudo_pred_qs[1,2:]],\n",
    "    fmt='o', color='brown',\n",
    "    label=f\"Pseudoinv pred.\", capsize=2, markersize=5, marker=\".\", linestyle=\":\", linewidth=2, zorder=2\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"Context\", fontsize=16)\n",
    "ax.set_xlim([2, context_len])\n",
    "ax.set_ylabel(\"Error\", fontsize=16)\n",
    "ax.legend(loc='upper right')\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.grid(True)\n",
    "ax.grid(which='minor', linestyle=':', linewidth=0.25, color='gray', alpha=0.5)\n",
    "ax.grid(which='major', linestyle='-', linewidth=0.35, color='gray', alpha=0.5)\n",
    "ax.set_ylim([2e-4, 1.2e0])\n",
    "\n",
    "# Adding a colorbar\n",
    "\n",
    "# After filling examples_list:\n",
    "examples_array = np.array(examples_list)\n",
    "# Compute boundaries: midpoints between each example count\n",
    "boundaries = np.zeros(len(examples_array) + 1)\n",
    "boundaries[1:-1] = (examples_array[:-1] + examples_array[1:]) / 2\n",
    "# Extend the first and last boundary\n",
    "boundaries[0] = examples_array[0] - (examples_array[1] - examples_array[0]) / 2\n",
    "boundaries[-1] = examples_array[-1] + (examples_array[-1] - examples_array[-2]) / 2\n",
    "\n",
    "# Select every other tick for the colorbar\n",
    "tick_indices = np.arange(0, len(examples_array), 2)\n",
    "cbar_ticks = examples_array[tick_indices]\n",
    "\n",
    "norm = BoundaryNorm(boundaries, combined_cmap.N)\n",
    "sm = plt.cm.ScalarMappable(cmap=combined_cmap, norm=norm)\n",
    "sm.set_array([])\n",
    "cbar = fig.colorbar(sm, ax=ax, boundaries=boundaries, ticks=cbar_ticks)\n",
    "cbar.set_label(\"# of Training Examples\", fontsize=14, rotation=270, labelpad=20)\n",
    "cbar.ax.set_yticklabels([f\"{int(x):.0e}\" for x in cbar_ticks])  # scientific notation\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "timestamp = time.strftime(\"%y%m%d_%H\")\n",
    "\n",
    "os.makedirs(\"../outputs/GPT2/\" + experiment_multi_train + \"/figures/context\", exist_ok=True)\n",
    "fig.savefig(\"../outputs/GPT2/\" + experiment_multi_train + f\"/figures/context/context_plot_{timestamp}.pdf\", format=\"pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "\n",
    "cmap = plt.cm.magma\n",
    "\n",
    "colors = cmap(np.linspace(0, 1, len(np.arange(context_len))))\n",
    "pseudo_colors = plt.cm.viridis(np.linspace(0, 1, 9))\n",
    "\n",
    "for context in np.arange(0, context_len):\n",
    "    train_convs = [tf_err_multi_trains[i][1][context] for i in range(len(tf_err_multi_trains))]\n",
    "\n",
    "    ax.plot(examples_list, train_convs, label=f\"_no_legend_\", linewidth=2, marker=\".\", markersize=2, color=colors[context])\n",
    "    if context < 8 and context > 1:\n",
    "        skip=5\n",
    "        examples_arr = np.array(examples_list)\n",
    "        y = np.full(examples_arr[::skip].shape, pseudo_pred_qs[1, context], dtype=float)\n",
    "        yerr_lower = np.full(examples_arr[::skip].shape, pseudo_pred_qs[1, context] - pseudo_pred_qs[0, context], dtype=float)\n",
    "        yerr_upper = np.full(examples_arr[::skip].shape, pseudo_pred_qs[2, context] - pseudo_pred_qs[1, context], dtype=float)\n",
    "        ax.errorbar(\n",
    "            examples_arr[::skip], y,\n",
    "            yerr=[yerr_lower, yerr_upper],\n",
    "            fmt='o', color=pseudo_colors[context],\n",
    "            label=f\"Pseudoinv pred. index {context}\", capsize=2, markersize=5, marker=\".\", linestyle=\":\", linewidth=2, zorder=2\n",
    "        )\n",
    "        # ax.hlines(y = pseudo_pred_qs[1, context], xmin=examples_list[0], xmax=examples_list[-1], color=pseudo_colors[context], linestyle='--', linewidth=2, label=f\"Pseudoinv pred. index {context}\")\n",
    "\n",
    "#place a red dotted vertical line at early_stop*batch_size*gpus\n",
    "early_stop = 122000\n",
    "ax.axvline(x=early_stop*batch_size*gpus, color='red', linestyle='--', linewidth=2, label='Early stop checkpoint')\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_xlabel(\"# of Training Examples\", fontsize=12)\n",
    "ax.set_ylabel(\"Error\", fontsize=12)\n",
    "ax.set_xlim([1e3*batch_size*gpus, max_ckpt*batch_size*gpus])\n",
    "ax.grid(True)\n",
    "ax.grid(which='minor', linestyle=':', linewidth='0.25', color='gray', alpha=0.5)\n",
    "ax.grid(which='major', linestyle='-', linewidth='0.35', color='gray', alpha=0.5)\n",
    "ax.legend(loc='lower left', fontsize=9)\n",
    "\n",
    "#add colorbar\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=5, vmax=context_len))\n",
    "sm.set_array([])\n",
    "cbar = fig.colorbar(sm, ax=ax)\n",
    "cbar.set_label(\"Context\", fontsize=12, rotation=270, labelpad=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "fig_dir = \"../outputs/GPT2/\" + experiment_multi_train + \"/figures/context\"\n",
    "os.makedirs(fig_dir, exist_ok=True)\n",
    "timestamp = time.strftime(\"%y%m%d_%H%M\")\n",
    "fig.savefig(f\"{fig_dir}/context_train_conv_plot_{timestamp}.pdf\", format=\"pdf\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## zero cut train conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valA = \"gaussA\"\n",
    "valC = \"_gauss_C\"\n",
    "state_dim = 10\n",
    "batch_size = 512 #512 #4096\n",
    "gpus=2\n",
    "num_val_sys = 25\n",
    "experiment1 = \"250114_202420.3c1184_multi_sys_trace_gaussA_state_dim_10_gauss_C_lr_1.584893192461114e-05_num_train_sys_40000\" # experiment to load\n",
    "experiment2 = \"250127_001511.3ac954_multi_sys_trace_zero_cut_gaussA_state_dim_10_gauss_C_lr_1.584893192461114e-05_num_train_sys_40000\"\n",
    "datasource = \"val\"\n",
    "nope = False\n",
    "single_system = False\n",
    "zero_cut = True\n",
    "\n",
    "\n",
    "compute_more_ckpts = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kal_ckpts = [180000, 90000]  #60000 #81000 #28500\n",
    "train_conv_plots([experiment1, experiment2], [\"Multi-cut Trained\", \"Zero-cut Trained\"], kal_ckpts, valA, valC, num_val_sys, compute_more_ckpts, 200, 1000, 180000, 1000, state_dim, single_system=single_system, nope=nope, batch_size=batch_size, gpus=gpus, zero_cut=zero_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
